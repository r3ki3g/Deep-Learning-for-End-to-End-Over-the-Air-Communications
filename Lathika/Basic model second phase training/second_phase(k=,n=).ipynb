{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rx_4n_2k.model as model\n",
    "import rx_4n_2k.__init__ as init\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "n = 2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 10\n",
    "\n",
    "weight_path = 'rx_4n_2k\\weights.h5'\n",
    "data_path = r'data.xlsx'\n",
    "\n",
    "best_model_name = 'best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_weights=True, debug=False):\n",
    "    m = model.create_model()\n",
    "    if load_weights:\n",
    "        loadWeights(m, debug=debug)\n",
    "    return m\n",
    "\n",
    "## Utility functions:\n",
    "\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "\n",
    "def loadWeights(model, filename=weight_path, debug=False):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Every layer is an h5 group. Ignore non-groups (such as /0)\n",
    "        for g in f:\n",
    "            if isinstance(f[g], h5py.Group):\n",
    "                group = f[g]\n",
    "                layerName = group.attrs['Name']\n",
    "                numVars = int(group.attrs['NumVars'])\n",
    "                if debug:\n",
    "                    print(\"layerName:\", layerName)\n",
    "                    print(\"    numVars:\", numVars)\n",
    "                # Find the layer index from its namevar\n",
    "                layerIdx = layerNum(model, layerName)\n",
    "                layer = model.layers[layerIdx]\n",
    "                if debug:\n",
    "                    print(\"    layerIdx=\", layerIdx)\n",
    "                # Every weight is an h5 dataset in the layer group. Read the weights \n",
    "                # into a list in the correct order\n",
    "                weightList = [0]*numVars\n",
    "                for d in group:\n",
    "                    dataset = group[d]\n",
    "                    varName = dataset.attrs['Name']\n",
    "                    shp     = intList(dataset.attrs['Shape'])\n",
    "                    weightNum = int(dataset.attrs['WeightNum'])\n",
    "                    # Read the weight and put it into the right position in the list\n",
    "                    if debug:\n",
    "                        print(\"    varName:\", varName)\n",
    "                        print(\"        shp:\", shp)\n",
    "                        print(\"        weightNum:\", weightNum)\n",
    "                    weightList[weightNum] = tf.constant(dataset[()], shape=shp)\n",
    "                # Assign the weights into the layer\n",
    "                for w in range(numVars):\n",
    "                    if debug:\n",
    "                        print(\"Copying variable of shape:\")\n",
    "                        print(weightList[w].shape)\n",
    "                    layer.variables[w].assign(weightList[w])\n",
    "                    if debug:\n",
    "                        print(\"Assignment successful.\")\n",
    "                        print(\"Set variable value:\")\n",
    "                        print(layer.variables[w])\n",
    "                # Finalize layer state\n",
    "                if hasattr(layer, 'finalize_state'):\n",
    "                    layer.finalize_state()\n",
    "\n",
    "def layerNum(model, layerName):\n",
    "    # Returns the index to the layer\n",
    "    layers = model.layers\n",
    "    for i in range(len(layers)):\n",
    "        if layerName==layers[i].name:\n",
    "            return i\n",
    "    print(\"\")\n",
    "    print(\"WEIGHT LOADING FAILED. MODEL DOES NOT CONTAIN LAYER WITH NAME: \", layerName)\n",
    "    print(\"\")\n",
    "    return -1\n",
    "\n",
    "def intList(myList): \n",
    "    # Converts a list of numbers into a list of ints.\n",
    "    return list(map(int, myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary_array(decimal_array,k):\n",
    "\n",
    "  binary_array = np.zeros((len(decimal_array), k), dtype=np.float32)\n",
    "\n",
    "  # Use bitwise AND operation with powers of 2 to get individual bits\n",
    "  for i, num in enumerate(decimal_array):\n",
    "    for j in range(k):\n",
    "      binary_array[i, k-j-1] = (num & (2**j)) != 0\n",
    "  return binary_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m inp_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mreal(inp_stream),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)),np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mimag(inp_stream),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m inp_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(inp_stream,(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn))  \u001b[38;5;66;03m# convert to a one demensional array\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m out_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m out_stream \u001b[38;5;241m=\u001b[39m decimal_to_binary_array(out_stream,k)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Spliting\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[87], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m inp_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mreal(inp_stream),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)),np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mimag(inp_stream),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m inp_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(inp_stream,(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn))  \u001b[38;5;66;03m# convert to a one demensional array\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m out_stream \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m      9\u001b[0m out_stream \u001b[38;5;241m=\u001b[39m decimal_to_binary_array(out_stream,k)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Spliting\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "decoder = load_model(load_weights=True, debug=False)\n",
    "\n",
    "# Getting data and setting up inputs and outputs to the model\n",
    "df = pd.read_excel(data_path)\n",
    "inp_stream = np.array([complex(i) for i in df.iloc[:, 0]])\n",
    "inp_stream = np.concatenate((np.reshape(np.real(inp_stream),(-1,1)),np.reshape(np.imag(inp_stream),(-1,1))),axis=1)\n",
    "inp_stream = np.reshape(inp_stream,(-1,2*n))  # convert to a one demensional array\n",
    "out_stream = np.array([int(i) for i in df.iloc[:, 1]])\n",
    "out_stream = decimal_to_binary_array(out_stream,k)\n",
    "# Spliting\n",
    "train_x,val_x = np.split(inp_stream,[int(0.75 * len(inp_stream))])\n",
    "train_y,val_y = np.split(out_stream,[int(0.75 * len(out_stream))])\n",
    "print(out_stream)\n",
    "print(len(out_stream))\n",
    "print(train_x,train_y)\n",
    "print(val_x,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 6\n  y sizes: 12\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train decoder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m decoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate),loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lathw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lathw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1950\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1943\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1944\u001b[0m         label,\n\u001b[0;32m   1945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1946\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1947\u001b[0m         ),\n\u001b[0;32m   1948\u001b[0m     )\n\u001b[0;32m   1949\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 6\n  y sizes: 12\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Train decoder\n",
    "decoder.compile(optimizer=Adam(learning_rate=learning_rate),loss=\"binary_crossentropy\",matrics = ['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience =n_epochs//10)\n",
    "mc = ModelCheckpoint(best_model_name,monitor='val_accuracy',mode='max',verbose=1,save_best_only=True)\n",
    "history = decoder.fit(train_x,train_y,validation_data = (val_x,val_y),epochs = n_epochs,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
