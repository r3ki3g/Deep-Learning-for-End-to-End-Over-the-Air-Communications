{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Automate to test different architectures >> select the best \n",
    "\n",
    "\n",
    "<b>pre-version: -</b>\n",
    "newstart-SD_w_RNN_tensorflow__cleaned(notebook_id=2024_04_16_02_31PM)-lathika'sUpdatedBlocks\n",
    "\n",
    "<hr/>\n",
    "<b>Steps :-</b>\n",
    "\n",
    "    1. Fixed pre-trained encoder same model for all tests (not trained either)\n",
    "    2. Fixed stochastic channel\n",
    "    3. Sequence decoder\n",
    "        Following need to have variable complexity\n",
    "        \n",
    "        a. Feature extractor \n",
    "        b. Phase estimator\n",
    "        c. Rx decoder\n",
    "        d. Internal slicer - boundaries\n",
    "        e. state width in RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSTgsilO1_ur"
   },
   "source": [
    "online version of this notebook : https://colab.research.google.com/drive/12iz5_mTOmTa0oyn5IZZYnEskVUonViVW#scrollTo=VSTgsilO1_ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.712987Z",
     "start_time": "2024-04-18T05:17:14.697553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eperiments on/Off\n",
    "APPLY_TANH_STATE =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.732672Z",
     "start_time": "2024-04-18T05:17:14.714175Z"
    },
    "id": "pWavhXJRGytK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.activations import relu, softmax, tanh\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNx0nzHGxCtg"
   },
   "source": [
    "## PARAMS\n",
    "<a name=\"params\">.</a>\n",
    "<a href=\"#datasyn\">go to data gen</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.748763Z",
     "start_time": "2024-04-18T05:17:14.732672Z"
    },
    "id": "eQvOU0nYbPdu"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "k = 4\n",
    "NUM_CHANNEL_USES = 7\n",
    "block_size = 320\n",
    "\n",
    "snr = 6 #9 for training\n",
    "\n",
    "model_training_num_of_frames = 10**3 #10**4\n",
    "model_validating_num_of_frames = 10**2 #10**3\n",
    "\n",
    "n_train = block_size * model_training_num_of_frames\n",
    "n_val   = block_size * model_validating_num_of_frames\n",
    "\n",
    "# Geanerating dataset\n",
    "model_output_num_of_frames = 10**5\n",
    "n_out = block_size * model_output_num_of_frames\n",
    "\n",
    "num_epoches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.772774Z",
     "start_time": "2024-04-18T05:17:14.749709Z"
    },
    "id": "8llGxbteuvUt"
   },
   "outputs": [],
   "source": [
    "SLICED_Y_LENGTH = 16\n",
    "BATCH_SIZE =  1\n",
    "\n",
    "# in teh feature extractor path \"f\" : design param\n",
    "# Our experiments have shown that even a\n",
    "# small number of features, e.g., F = 4, significantly improves\n",
    "# the performance.\n",
    "N_FEATURES_EXTRACTED = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.792373Z",
     "start_time": "2024-04-18T05:17:14.772774Z"
    },
    "id": "AU4v5qeU_LTW"
   },
   "outputs": [],
   "source": [
    "def R2C(a):\n",
    "\n",
    "    aa = tf.cast(tf.reshape(a,shape=(BATCH_SIZE,-1,2)),tf.float32)\n",
    "\n",
    "    aaa = tf.complex(aa[:,:,0],aa[:,:,1])\n",
    "    return aaa\n",
    "\n",
    "def C2R(a):\n",
    "    real, imag = tf.expand_dims(tf.math.real(a),axis=2) ,tf.expand_dims(tf.math.imag(a), axis=2)\n",
    "    R = tf.concat((real,imag),axis=2)\n",
    "    R = tf.reshape(R , (BATCH_SIZE,-1)  )\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9Oj3oPvZcHV"
   },
   "source": [
    "# Stochastic Channel Model & Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsd5N4qhfV0O"
   },
   "source": [
    "### Additional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.807965Z",
     "start_time": "2024-04-18T05:17:14.793407Z"
    },
    "id": "a73PhgGbfYlk"
   },
   "outputs": [],
   "source": [
    "class L2Normalization(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(L2Normalization, self).__init__(**kwargs)\n",
    "    def call(self, inputs):\n",
    "        out = tf.nn.l2_normalize(inputs, axis=-1)\n",
    "        #print(\"normalize output shape = \",out.shape)\n",
    "        return out\n",
    "    def get_config(self):\n",
    "        return super(L2Normalization, self).get_config()\n",
    "\n",
    "\n",
    "def generate_nakagami_samples(m, omega):\n",
    "    nakagami_amp_vec = nakagami.rvs(m,omega,size =  NUM_CHANNEL_USES)   # Same gain for the real part and the imaginary part\n",
    "    nakagami_phase_vec = np.random.uniform(low=0.0, high=2*np.pi, size = NUM_CHANNEL_USES)    # phase shift will effect the complex number\n",
    "    nakagami_for_real = np.reshape(nakagami_amp_vec*np.cos(nakagami_phase_vec),(-1,1))\n",
    "    nakagami_for_imag = np.reshape(nakagami_amp_vec*np.sin(nakagami_phase_vec),(-1,1))\n",
    "    fading_vec = np.reshape(np.concatenate((nakagami_for_real,nakagami_for_imag),axis=1),(1,-1))[0]\n",
    "    return  tf.constant(fading_vec, dtype=tf.float32)\n",
    "\n",
    "class NakagamiNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, distribution_params, **kwargs):\n",
    "        super(NakagamiNoiseLayer, self).__init__(**kwargs)\n",
    "        self.distribution_params = distribution_params\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "      fading = generate_nakagami_samples(m = self.distribution_params[\"m\"],\n",
    "                                        omega = self.distribution_params[\"omega\"])\n",
    "      return inputs * fading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1ejiEuHf1B0"
   },
   "source": [
    "### Stochastic channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.825243Z",
     "start_time": "2024-04-18T05:17:14.809920Z"
    },
    "id": "Tm8yRX0xa_ul"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "channel_parameters = {\n",
    "    \"r\"        : 4,             # For upsampling -> number of complex samples per symbol\n",
    "    \"roll_off\" : 0.35,          # Roll off factor\n",
    "    \"num_taps\" : 31,            # L -> Number of taps (odd) for RRC filter\n",
    "    \"f_s\"      : 25e4,          # Add what is in the physical implementation\n",
    "    \"T_bound\"  : 1/25e4,        # 1/f_s Go through the resharch paper Deep Learning Based Communication Over the Air  (content under table 1)\n",
    "    \"time_delay\" : np.random.uniform(-1,1), # To convert the time delay into discrete domain, time dilay is giving relative to the sampling period\n",
    "    \"CFO\"      : 5e3,           # Observe from the physical implementation\n",
    "    \"CFO_std\"  : 5e3/25e4,      # CFO/f_s\n",
    "    \"snr\"      : 6,             # noise power will be calculating assuming transmittting power of 1\n",
    "    \"phase_off\": uniform.rvs(scale = 2*np.pi)  # constant for one channel input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.853463Z",
     "start_time": "2024-04-18T05:17:14.825243Z"
    },
    "id": "gA_f6Q7cZiUN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Making the stochasticChannelLayer\n",
    "\n",
    "# function to create the complex values\n",
    "def real_to_complex_tensor(inp_tensor):\n",
    "  inp_tensor = tf.reshape(inp_tensor, [-1, 2])\n",
    "  real_part = inp_tensor[:, 0]\n",
    "  imag_part = inp_tensor[:, 1]\n",
    "  complex_tensor = tf.complex(real_part, imag_part)\n",
    "  return complex_tensor\n",
    "\n",
    "def complex_to_real_tensor(inp_tensor):\n",
    "   real_part , imag_part = tf.math.real(inp_tensor), tf.math.imag(inp_tensor)\n",
    "   real_part = tf.reshape(real_part,[-1,1])\n",
    "   imag_part = tf.reshape(imag_part,[-1,1])\n",
    "   return tf.reshape(tf.concat([real_part,imag_part],1),[-1])\n",
    "\n",
    "# Upsample\n",
    "def upsampling(inp,r):\n",
    "  com_reshape = tf.reshape(inp,[-1,1])\n",
    "  padding = tf.constant([[0,0],[0,r-1]])\n",
    "  upsampled = tf.pad(com_reshape,padding,\"CONSTANT\")\n",
    "  return tf.reshape(upsampled,[-1])\n",
    "\n",
    "# Normalized RRC with time shift\n",
    "def NRRC_filter(num_taps, roll_off, time_delay):\n",
    "  t = np.linspace(-(num_taps-1)/2,(num_taps-1)/2,num_taps) - time_delay\n",
    "  eps = np.finfo(float).eps # Small epsilon to avoid divisiomn by zero\n",
    "  pi = np.pi\n",
    "  def RRC_filter_coff(t):\n",
    "    if abs(t) < eps:  # For t==0\n",
    "      return 1.0 - roll_off + (4*roll_off/pi)\n",
    "    elif roll_off != 0 and (abs(t-1/(4*roll_off))<eps or abs(t+1/(4*roll_off))<eps):\n",
    "      return (roll_off/np.sqrt(2))*(1 + 2/pi)*np.sin(pi/(4*roll_off)) + (1- 2/pi)*np.cos(pi/(4*roll_off))\n",
    "    else:\n",
    "      nu = np.sin(pi*t*(1-roll_off)) + 4*roll_off*t*np.cos(pi*t*(1+roll_off))\n",
    "      den = pi*t*(1-(4*roll_off*t)**2)\n",
    "      return nu/(den + eps)\n",
    "  filter_coff = np.array([RRC_filter_coff(T) for T in t])\n",
    "  NRRC_filter_coff = filter_coff / np.sum(np.abs(filter_coff))\n",
    "  #print(f\"Time_delay = {time_delay}\")\n",
    "  #plt.stem(t,NRRC_filter_coff)  # Plot for visualization\n",
    "  return tf.constant(NRRC_filter_coff,dtype = tf.float32)\n",
    "\n",
    "# Phase offset\n",
    "def PhaseOffset_vec(batch_size,NUM_CHANNEL_USES,num_taps,r,CFO_std,phase_off):\n",
    "  l = batch_size*r*NUM_CHANNEL_USES+num_taps-1\n",
    "  CFO_off = 0.1*CFO_std # truncnorm.rvs(-1.96,1.96)*CFO_std  # boundaries will be selected for 95% confidence\n",
    "  #print(\"CFO_off =\",CFO_off)\n",
    "  #print(\"Phase offset = \",phase_off)                                          # CFO_min and CFO_max (boundaries) will be selected for 95% confidence\n",
    "  exp_vec = []\n",
    "  for i in range(l):\n",
    "    exp_vec.append(tf.math.exp(tf.constant([0+(2*np.pi*i*CFO_off+phase_off)*1j],dtype=tf.complex64)))\n",
    "  return tf.reshape(tf.stack(exp_vec),[-1])\n",
    "\n",
    "\n",
    "class UpsamplingLayer(keras.layers.Layer):\n",
    "    def __init__(self, r =channel_parameters['r']):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "    def call(self,inputs):\n",
    "       return upsampling(inputs,self.r)\n",
    "\n",
    "class PulseShaping(keras.layers.Layer):\n",
    "    def __init__(self,num_taps,roll_off,time_delay):\n",
    "      super().__init__()\n",
    "      self.nrrc_filter = NRRC_filter(num_taps,roll_off,time_delay)\n",
    "      self.nrrc_filter = tf.reshape(self.nrrc_filter,[num_taps,1,1])\n",
    "      self.num_taps = num_taps\n",
    "    def call(self, inputs):\n",
    "      padding_size = self.num_taps // 2\n",
    "      paddings = tf.constant([[padding_size, padding_size]])\n",
    "      real_part = tf.pad(tf.math.real(inputs), paddings, \"CONSTANT\")\n",
    "      imag_part = tf.pad(tf.math.imag(inputs), paddings, \"CONSTANT\")\n",
    "      real_part = tf.reshape(real_part,[1,-1,1])\n",
    "      imag_part = tf.reshape(imag_part,[1,-1,1])\n",
    "      real_conv = tf.nn.conv1d(real_part,self.nrrc_filter,stride=1,padding=\"SAME\")\n",
    "      imag_conv = tf.nn.conv1d(imag_part,self.nrrc_filter,stride=1,padding=\"SAME\")\n",
    "      real_conv = tf.reshape(real_conv,[-1])\n",
    "      imag_conv = tf.reshape(imag_conv,[-1])\n",
    "      return tf.complex(real_conv,imag_conv)\n",
    "\n",
    "class PhaseOffset(keras.layers.Layer):\n",
    "    def __init__(self,batch_size,NUM_CHANNEL_USES,num_taps,r,CFO_std,phase_off):\n",
    "      super().__init__()\n",
    "      self.batch_size = batch_size\n",
    "      self.num_channel_uses = NUM_CHANNEL_USES\n",
    "      self.num_taps = num_taps\n",
    "      self.r = r\n",
    "      self.CFO_std = CFO_std\n",
    "      self.phase_off = phase_off\n",
    "    def call(self,inputs):\n",
    "       return inputs * PhaseOffset_vec(self.batch_size, self.num_channel_uses,self.num_taps,self.r,self.CFO_std, self.phase_off)\n",
    "\n",
    "class StochasticChannelLayer(keras.layers.Layer):\n",
    "    \"\"\"This channel will output 1D tensor.\n",
    "        channel_parameters ---> custom class for parameters store\n",
    "                                channel_parameters = {\n",
    "                                    \"r\"        : 4,             # For upsampling -> number of complex samples per symbol\n",
    "                                    \"roll_off\" : 0.35,          # Roll off factor\n",
    "                                    \"num_taps\" : 31,            # L -> Number of taps (odd) for RRC filter\n",
    "                                    \"f_s\"      : 25e4,          # Add what is in the physical implementation\n",
    "                                    \"T_bound\"  : 1/25e4,        # 1/f_s Go through the resharch paper Deep Learning Based Communication Over the Air  (content under table 1)\n",
    "                                    \"time_delay\" : np.random.uniform(-1,1), # To convert the time delay into discrete domain, time dilay is giving relative to the sampling period\n",
    "                                    \"CFO\"      : 5e3,           # Observe from the physical implementation\n",
    "                                    \"CFO_std\"  : 5e3/25e4,      # CFO/f_s\n",
    "                                    \"snr\"      : 6,             # noise power will be calculating assuming transmittting power of 1\n",
    "                                    \"phase_off\": uniform.rvs(scale = 2*np.pi)  # constant for one channel input\n",
    "                                }\n",
    "        r ----------> upsampling constant (number of complex samples per symbol)\n",
    "        time_delay -> uniformly distributed time delay between (-1,1), discrete domain,\n",
    "                      time dilay is giving relative to the sampling period\n",
    "        CFO_std ----> CFO_frequency / sampling_frequency is taken as the standared deviation\n",
    "        snr --------> snr for AWGN channel\n",
    "        output_shape -> None - output_shape is 1D tensor for sequence decoder, or give an output shape prefer \"\"\"\n",
    "    def __init__(self, NUM_CHANNEL_USES,batch_size,channel_parameters):\n",
    "        super().__init__()\n",
    "        self.UpSamplingLayer_inst = UpsamplingLayer(r)\n",
    "        self.PulseShaping_inst = PulseShaping(channel_parameters['num_taps'],channel_parameters['roll_off'],channel_parameters['time_delay'])\n",
    "        self.PhaseOffset_inst = PhaseOffset(batch_size,NUM_CHANNEL_USES,channel_parameters['num_taps'],channel_parameters['r'],channel_parameters['CFO_std'],channel_parameters['phase_off'])\n",
    "        self.AWGNlayer = keras.layers.GaussianNoise(stddev = np.sqrt(1/10**(channel_parameters['snr']/10)))\n",
    "    def call(self, inputs):\n",
    "      inputs = tf.reshape(inputs,[-1])\n",
    "      inputs = real_to_complex_tensor(inputs)\n",
    "      x = self.UpSamplingLayer_inst(inputs)\n",
    "      x = self.PulseShaping_inst(x)\n",
    "      x = self.PhaseOffset_inst(x)\n",
    "      x = complex_to_real_tensor(x)\n",
    "      x = self.AWGNlayer(x)\n",
    "      #print(\"StochasticChannelLayer output shape = \",x.shape)\n",
    "      return x\n",
    "\n",
    "\n",
    "# Stochastic channel model\n",
    "class StochasticChannelModel(keras.Model):\n",
    "    \"\"\"This channel will output 1D tensor.\n",
    "        channel_parameters ---> custom class for parameters store\n",
    "                                channel_parameters = {\n",
    "                                    \"r\"        : 4,             # For upsampling -> number of complex samples per symbol\n",
    "                                    \"roll_off\" : 0.35,          # Roll off factor\n",
    "                                    \"num_taps\" : 31,            # L -> Number of taps (odd) for RRC filter\n",
    "                                    \"f_s\"      : 25e4,          # Add what is in the physical implementation\n",
    "                                    \"T_bound\"  : 1/25e4,        # 1/f_s Go through the resharch paper Deep Learning Based Communication Over the Air  (content under table 1)\n",
    "                                    \"time_delay\" : np.random.uniform(-1,1), # To convert the time delay into discrete domain, time dilay is giving relative to the sampling period\n",
    "                                    \"CFO\"      : 5e3,           # Observe from the physical implementation\n",
    "                                    \"CFO_std\"  : 5e3/25e4,      # CFO/f_s\n",
    "                                    \"snr\"      : 6,             # noise power will be calculating assuming transmittting power of 1\n",
    "                                    \"phase_off\": uniform.rvs(scale = 2*np.pi)  # constant for one channel input\n",
    "                                }\n",
    "        r ----------> upsampling constant (number of complex samples per symbol)\n",
    "        time_delay -> uniformly distributed time delay between (-1,1), discrete domain,\n",
    "                      time dilay is giving relative to the sampling period\n",
    "        CFO_std ----> CFO_frequency / sampling_frequency is taken as the standared deviation\n",
    "        snr --------> snr for AWGN channel\n",
    "        output_shape -> None - output_shape is 1D tensor for sequence decoder, or give an output shape prefer \"\"\"\n",
    "    def __init__(self, NUM_CHANNEL_USES,batch_size,channel_parameters):\n",
    "        super().__init__()\n",
    "        self.UpSamplingLayer_inst = UpsamplingLayer(channel_parameters['r'])\n",
    "        self.PulseShaping_inst = PulseShaping(channel_parameters['num_taps'],channel_parameters['roll_off'],channel_parameters['time_delay'])\n",
    "        self.PhaseOffset_inst = PhaseOffset(batch_size,NUM_CHANNEL_USES,channel_parameters['num_taps'],channel_parameters['r'],channel_parameters['CFO_std'],channel_parameters['phase_off'])\n",
    "        self.AWGNlayer = keras.layers.GaussianNoise(stddev = np.sqrt(1/10**(channel_parameters['snr']/10)))\n",
    "    def call(self, inputs):\n",
    "      inputs = tf.reshape(inputs,[-1])\n",
    "      inputs = real_to_complex_tensor(inputs)\n",
    "      x = self.UpSamplingLayer_inst(inputs)\n",
    "      x = self.PulseShaping_inst(x)\n",
    "      x = self.PhaseOffset_inst(x)\n",
    "      x = complex_to_real_tensor(x)\n",
    "      x = self.AWGNlayer(x)\n",
    "      #print(\"StochasticChannelLayer output shape = \",x.shape)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.868739Z",
     "start_time": "2024-04-18T05:17:14.854875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decoder mask layer\n",
    "\n",
    "class PulseShaping_Dec(keras.layers.Layer):\n",
    "    def __init__(self,num_taps,r,roll_off,time_delay):\n",
    "      super().__init__()\n",
    "      self.nrrc_filter = NRRC_filter(num_taps,roll_off,time_delay)\n",
    "      self.nrrc_filter = tf.reshape(self.nrrc_filter,[num_taps,1,1])\n",
    "      self.num_taps = num_taps\n",
    "      self.r =r\n",
    "    def call(self, inputs):\n",
    "      inputs = tf.reshape(inputs,[1,-1,1])\n",
    "      inp_conv = tf.nn.conv1d(inputs,self.nrrc_filter,stride=self.r,padding=\"VALID\")\n",
    "      inp_conv = tf.reshape(inp_conv,[-1])\n",
    "      return inp_conv\n",
    "\n",
    "\n",
    "class DecoderMaskLayer(keras.layers.Layer):\n",
    "    def __init__(self,channel_parameters,NUM_CHANNEL_USES):\n",
    "        super().__init__()\n",
    "        # self.Convo = PulseShaping_Dec(channel_parameters['num_taps'],channel_parameters['r'],channel_parameters['roll_off'],channel_parameters['time_delay'])\n",
    "        self.Convo = tf.keras.layers.Conv1D(1,channel_parameters['num_taps'],strides=channel_parameters['r'], padding = 'valid',activation = 'relu',use_bias=True)\n",
    "        self.channel_uses = NUM_CHANNEL_USES\n",
    "    def call(self,inputs):\n",
    "        inp = tf.reshape(inputs,[-1,2])\n",
    "        real_part, imag_part = inp[:,0],inp[:,1]\n",
    "        vec_shape = real_part.shape[0]\n",
    "        #print(\"real shape\",real_part.shape)\n",
    "        real_part, imag_part = tf.reshape(real_part,[1,vec_shape,1]), tf.reshape(imag_part,[1,vec_shape,1])\n",
    "        real_part = tf.reshape(self.Convo(real_part),[-1,1])\n",
    "        imag_part = tf.reshape(self.Convo(imag_part),[-1,1])\n",
    "        #print(\"real shape after conv \",real_part.shape)\n",
    "        outputs = tf.concat([real_part,imag_part],1)\n",
    "        return tf.reshape(outputs,[-1,2*self.channel_uses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG_PMHaC_Nbd"
   },
   "source": [
    "## Main Blocks in the Sequence Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.884409Z",
     "start_time": "2024-04-18T05:17:14.870679Z"
    },
    "id": "Mx-vTDXeUVmT"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(Model):\n",
    "    def __init__(self,n_front_dense=1,state_width=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"r3 01 statewidth init:\",state_width)\n",
    "\n",
    "        self.cf1 = Sequential()\n",
    "        for i in range(n_front_dense):\n",
    "            self.cf1.add(Dense(256,name=f\"r3->FeatureExtractor->cf1->{i}\"))\n",
    "            self.cf1.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "        self.cf2 = Dense(N_FEATURES_EXTRACTED,name=\"r3->FeatureExtractor->cf2\")\n",
    "\n",
    "        self.cf_state = Dense(state_width,name=\"featureExtractor_stateFC\")\n",
    "\n",
    "    def call(self,sliced_y,prev_state_FE):\n",
    "       \n",
    "\n",
    "        # combine the sliced_y and prev_state_FE\n",
    "        sliced_y = tf.concat([sliced_y,prev_state_FE],axis=1)\n",
    "        print(\"r3 02 prev_state_FE shape in call:\",prev_state_FE.shape)\n",
    "        print(\"r3 03 sliced_y shape in call:\",sliced_y.shape)\n",
    "\n",
    "        sliced_y = self.cf1(sliced_y)\n",
    "        \n",
    "\n",
    "        state_FE = self.cf_state(sliced_y) # state calculated here\n",
    "        if APPLY_TANH_STATE:\n",
    "            state_FE = tanh(state_FE)\n",
    "\n",
    "        sliced_y = self.cf2(state_FE)\n",
    "\n",
    "        return sliced_y,state_FE\n",
    "\n",
    "class PhaseEstimator(Model):\n",
    "    def __init__(self,n_front_dense=1,state_width=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cf1 = Sequential()\n",
    "        for i in range(n_front_dense):\n",
    "            self.cf1.add(Dense(256,name=f\"r3->PhaseEstimator->cf1->{i}\"))\n",
    "            self.cf1.add(tf.keras.layers.Activation('relu'))\n",
    "        \n",
    "        self.cf2 = Dense(2,name=\"r3->PhaseEstimator->cf2\")\n",
    "\n",
    "        self.cf_state = Dense(state_width,name=\"PhaseEstimator_stateFC\")\n",
    "\n",
    "\n",
    "    def call(self,sliced_y,prev_state_PE):\n",
    "        # combine sliced_y and prev_state_PE\n",
    "        sliced_y = tf.concat([sliced_y,prev_state_PE],axis=1)\n",
    "        sliced_y = self.cf1(sliced_y)\n",
    "        sliced_y = relu(sliced_y)\n",
    "\n",
    "        state_PE = self.cf_state(sliced_y) # state calculated here\n",
    "        if APPLY_TANH_STATE:\n",
    "            state_PE = tanh(state_PE)\n",
    "\n",
    "        sliced_y = self.cf2(state_PE)\n",
    "\n",
    "        return sliced_y,state_PE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Rx_Decoder_new(Model):\n",
    "    def __init__(self,n_front_dense=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cf1 = Sequential()\n",
    "        for i in range(n_front_dense):\n",
    "            self.cf1.add( Dense(256,name=f\"r3->Rx_Decoder_new->cf1->{i}\"))\n",
    "            self.cf1.add(tf.keras.layers.Activation('relu'))\n",
    "       \n",
    "        self.cf3 = Dense(16,name=\"final_out_cf3\")\n",
    "\n",
    "        # useless\n",
    "        #self.cf4_state = Dense(8,name=\"state_dense_cf4\")\n",
    "\n",
    "    def call(self,concat):\n",
    "\n",
    "        concat = self.cf1(concat) # relu applied inside\n",
    "        \n",
    "        \n",
    "        # state = self.cf4_state(concat)\n",
    "        concat = self.cf3(concat)\n",
    "\n",
    "\n",
    "\n",
    "        # do not use softmax here : put from logit  = True in loss func\n",
    "        # concat = softmax(concat)\n",
    "\n",
    "        return concat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InternalSlicer(Model):\n",
    "    def __init__(self,l1,l2,complex_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the slice boundaries\n",
    "        mid = complex_length // 2\n",
    "        self.start = mid - l1\n",
    "        self.end = mid + l2 + 1\n",
    "\n",
    "    def call(self,sliced_y):\n",
    "\n",
    "        ret = C2R(  R2C(sliced_y)[:, self.start:self.end]  )\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def phase_multiply(internally_sliced_y,estimated_phase):\n",
    "    # (a,b) * (c,d) = (ac-bd,ad+bc)\n",
    "\n",
    "    internally_sliced_y_complex = R2C(internally_sliced_y)\n",
    "    estimated_phase_complex = R2C(estimated_phase)\n",
    "    phase_corrected_complex = tf.multiply(estimated_phase_complex , internally_sliced_y_complex)\n",
    "\n",
    "    phase_corrected = C2R(phase_corrected_complex)\n",
    "    return phase_corrected\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T9CJbZr_UgK"
   },
   "source": [
    "## Fake data syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.900185Z",
     "start_time": "2024-04-18T05:17:14.885409Z"
    },
    "id": "mj1GemkdxHCs"
   },
   "outputs": [],
   "source": [
    "# generate fake data\n",
    "\n",
    "# m = 512* 2** 2\n",
    "# X = tf.random.normal(shape=(block_size,SLICED_Y_LENGTH),\n",
    "#                      mean=0,\n",
    "#                      stddev=1)\n",
    "\n",
    "# Y = tf.random.uniform(shape=(m,1),\n",
    "#                       minval=0,\n",
    "#                       maxval=16,\n",
    "#                       dtype=tf.int32)\n",
    "# Y = keras.utils.to_categorical(Y,16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xqcXAB_XdM"
   },
   "source": [
    "## Main Model : Sequence Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.915941Z",
     "start_time": "2024-04-18T05:17:14.901180Z"
    },
    "id": "M_bSkx9vHEAd"
   },
   "outputs": [],
   "source": [
    "# sequence decoder\n",
    "\n",
    "\n",
    "class SequenceDecoder(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 take_prev_phase_state=False,\n",
    "                 n_front_dense_FE=1,\n",
    "                 n_front_dense_PE=1,\n",
    "                 n_front_dense_RxDec=2,\n",
    "                 state_width=8):\n",
    "        \n",
    "        super(SequenceDecoder,self).__init__()\n",
    "\n",
    "        self.take_prev_phase_state = take_prev_phase_state\n",
    "        self.state_width = state_width\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(n_front_dense=n_front_dense_FE,state_width=state_width)\n",
    "        self.phase_estimator = PhaseEstimator(n_front_dense=n_front_dense_PE,state_width=state_width)\n",
    "        self.internal_slicer = InternalSlicer(l1=3,l2=3,complex_length=SLICED_Y_LENGTH//2)\n",
    "\n",
    "        if take_prev_phase_state:\n",
    "            self.rx_decoder_RNN = Rx_Decoder_new(n_front_dense=n_front_dense_RxDec)\n",
    "        else:\n",
    "            raise Exception(\"How  come here??\")\n",
    "            #self.rx_decoder = Rx_Decoder_old()\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,\n",
    "             sliced_y,\n",
    "             prev_state_FE=None,\n",
    "             prev_state_PE=None):\n",
    "        \n",
    "        \n",
    "\n",
    "        if prev_state_PE is None:\n",
    "            print(\" How this none?\")\n",
    "            prev_state_PE = tf.constant(tf.zeros((block_size,self.state_width)))\n",
    "        if prev_state_FE is None:\n",
    "            print(\" How this none?\")\n",
    "            prev_state_FE = tf.constant(tf.zeros((block_size,self.state_width)))\n",
    "\n",
    "\n",
    "        # RNN conn starts here\n",
    "\n",
    "        output_FE = self.feature_extractor(sliced_y,prev_state_FE=prev_state_FE)\n",
    "        extracted_features,state_FE = output_FE[0], output_FE[1]\n",
    "\n",
    "\n",
    "\n",
    "        output_PE = self.phase_estimator(sliced_y,prev_state_PE=prev_state_PE)\n",
    "        estimated_phase,state_PE = output_PE[0], output_PE[1]\n",
    "\n",
    "        # RNN conn ends here\n",
    "\n",
    "        internally_sliced_y = self.internal_slicer(sliced_y)\n",
    "\n",
    "\n",
    "\n",
    "        phase_corrected_ = phase_multiply(internally_sliced_y,estimated_phase)\n",
    "\n",
    "        concat = tf.concat((extracted_features,phase_corrected_),axis=1)\n",
    "        if self.take_prev_phase_state:\n",
    "            st_hat = self.rx_decoder_RNN(concat)\n",
    "            return (st_hat,state_FE,state_PE)\n",
    "        else:\n",
    "            raise Exception(\"How came here????\")\n",
    "            print(\"--PROBLEM--\")\n",
    "            st_hat = self.rx_decoder(concat)\n",
    "            return st_hat\n",
    "\n",
    "\n",
    "\n",
    "    def custom_train(self,X,Y,epochs=1): # X =  vertically stacked sliced_y, y = message index\n",
    "\n",
    "        raise Exception(\"This function need to be changed\")\n",
    "        \n",
    "        # tarin per each time step\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            # temp_prev_state_PE = [tf.constant(tf.zeros((X.shape[0],8)))] #append the last PE state here\n",
    "            # temp_prev_state_FE = [tf.constant(tf.zeros((X.shape[0],8)))] #append the last FE state here\n",
    "\n",
    "            temp_prev_state_PE = tf.constant(tf.zeros((1,8)))\n",
    "            temp_prev_state_FE = tf.constant(tf.zeros((1,8)))\n",
    "\n",
    "            loss_acc = 0\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "                print(f\"iterration : {i}\")\n",
    "                x =  tf.expand_dims(X[i,:],axis=0)\n",
    "\n",
    "                y = tf.expand_dims(Y[i,:],axis=0)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = self.call(x,\n",
    "                                       prev_state_PE=temp_prev_state_PE,\n",
    "                                       prev_state_FE=temp_prev_state_FE)\n",
    "\n",
    "                    st_hat,state_FE,state_PE = output[0], output[1], output[2]\n",
    "                    loss = self.compiled_loss(y,st_hat)\n",
    "\n",
    "                    #temp_prev_state = state ###### assign add dala balanna\n",
    "\n",
    "                    temp_prev_state_FE = (state_FE)\n",
    "                    temp_prev_state_PE = (state_PE)\n",
    "\n",
    "                grads = tape.gradient(loss,self.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads,self.trainable_variables))\n",
    "\n",
    "                loss_acc += loss.numpy() / X.shape[0] # take the mean\n",
    "                print(\"loss (individual): \", loss.numpy())\n",
    "            print(f'Epoch  : {_}/{epochs} --> Loss = {loss_acc}')\n",
    "\n",
    "        # returning the final batch's loss\n",
    "        return loss_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T06:45:05.437856Z",
     "start_time": "2024-04-15T06:45:05.422233Z"
    },
    "id": "jq24YZr4baut"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.932535Z",
     "start_time": "2024-04-18T05:17:14.916946Z"
    },
    "id": "KqIhonChiDnM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the SD\n",
    "# tested and worked\n",
    "# mySD =   SequenceDecoder(take_prev_phase_state=True)\n",
    "\n",
    "# mySD.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "#              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# mySD.custom_train(X,Y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.948588Z",
     "start_time": "2024-04-18T05:17:14.933620Z"
    },
    "id": "_rXUaAKBtGjj"
   },
   "outputs": [],
   "source": [
    "# mySD.build((2048,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft4zIJOAb70O"
   },
   "source": [
    "# Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.972668Z",
     "start_time": "2024-04-18T05:17:14.950576Z"
    },
    "id": "t3LnggrMdYqV"
   },
   "outputs": [],
   "source": [
    "AWGN_std = np.sqrt(1/10**(snr/10))\n",
    "act_func = 'tanh' # 'relu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:14.991640Z",
     "start_time": "2024-04-18T05:17:14.972668Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "4hLPpWIedf60",
    "outputId": "8341a7e9-8951-4f82-f9ee-2ba4bf06660a"
   },
   "outputs": [],
   "source": [
    "# # Encoder\n",
    "# Encoder = Sequential([\n",
    "#                     Dense(2**k, activation=act_func,input_shape=(2**k,)),#Dense(2**k, activation=act_func,input_shape=(k,)),\n",
    "#                     Dense(2**k, activation=act_func),\n",
    "#                     Dense(2*NUM_CHANNEL_USES, activation='linear',name=\"Encode_last_dense\"),\n",
    "#                     L2Normalization(name=\"normalization_layer\"),\n",
    "# ])\n",
    "\n",
    "# # Channel\n",
    "# Stochastic_channel = StochasticChannelModel(NUM_CHANNEL_USES,block_size,r,roll_off,L,time_delay,CFO_std,snr)\n",
    "\n",
    "# # Sequence decoder\n",
    "# Seq_decoder = SequenceDecoder(take_prev_phase_state=True)\n",
    "\n",
    "# # Auto encoder\n",
    "# Autoencoder = Sequential([\n",
    "#     Encoder,\n",
    "#     Stochastic_channel,\n",
    "#     # Seq_decoder\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.007120Z",
     "start_time": "2024-04-18T05:17:14.992643Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "D6QP70azZKLR",
    "outputId": "3bddf272-7540-429e-f177-585c26f9e1c3"
   },
   "outputs": [],
   "source": [
    "# Autoencoder.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "#                     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "\n",
    "# # cxannot connect the RNN for a whoe batch\n",
    "\n",
    "# # history = Autoencoder.fit(x_train,\n",
    "# #                           y_train,\n",
    "# #                           batch_size=block_size,\n",
    "# #                           epochs=num_epoches,\n",
    "# #                           verbose=2,\n",
    "# #                           validation_data=(x_val,y_val))\n",
    "\n",
    "# train_history = Autoencoder.custom_train(x_train,y_train,epochs=1)\n",
    "# print(\"train_history\", train_history)\n",
    "\n",
    "\n",
    "# def calc_block_accuracy(preds,y_val):\n",
    "#     n_bits_per_block = preds.shape[1]\n",
    "#     n_correct_bits = np.sum(preds == y_val,axis=1)\n",
    "#     block_accuracy = np.mean(n_correct_bits == n_bits_per_block)\n",
    "#     return block_accuracy\n",
    "\n",
    "# preds = AE.predict(x_val,batch_size=block_size)>0.5\n",
    "# accuracy =  calc_block_accuracy(preds,y_val)\n",
    "# print(f\"validation accuracy = {accuracy}\")\n",
    "# print(f\"snr = {snr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "420o_lm0p9b9"
   },
   "source": [
    "<a href=\"#params\">go toparams</a><br/>\n",
    "\n",
    "<a name=\"datasyn\">Generate Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.022453Z",
     "start_time": "2024-04-18T05:17:15.008112Z"
    },
    "id": "zhVsDdfPpM6V"
   },
   "outputs": [],
   "source": [
    "# synthesize some data\n",
    "x_train = tf.cast(tf.random.uniform((block_size,),minval=0,maxval=2**k),\n",
    "                  (tf.int32))\n",
    "\n",
    "y_train = tf.expand_dims(x_train,axis=1)\n",
    "\n",
    "x_train =  tf.one_hot(x_train,depth=2**k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPSeSA6tq-4r"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.042722Z",
     "start_time": "2024-04-18T05:17:15.022453Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d5-uKR_o-71",
    "outputId": "a605d800-01cf-42be-f1b8-1e947919655c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (320, 16)\n",
      "y_train.shape:  (320, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape: \",x_train.shape)\n",
    "print(\"y_train.shape: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.072295Z",
     "start_time": "2024-04-18T05:17:15.042722Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def ExternalSlicer(input_vec,padding=30,gamma=4):\n",
    "    '''\n",
    "    input_vec: should be the real and imag parts separately.\n",
    "    padding  : how much the window should expand per each side\n",
    "    '''\n",
    "    assert len(input_vec.shape) == 1, \"Need 1D vector. Cannot process multiple frames at once\"\n",
    "    assert (input_vec.shape[0]-2*padding ) / (2 * NUM_CHANNEL_USES * gamma) == block_size\n",
    "    \n",
    "    output_array = []\n",
    "    \n",
    "    for i in range(block_size):\n",
    "        jump_size = (2 * NUM_CHANNEL_USES * gamma) # how many to jump\n",
    "        start = jump_size * i\n",
    "        end = start + jump_size + padding * 2\n",
    "        output_array.append(input_vec[start:end])\n",
    "    \n",
    "    return tf.stack(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.088115Z",
     "start_time": "2024-04-18T05:17:15.072295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17980 %(320 * 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.112491Z",
     "start_time": "2024-04-18T05:17:15.089131Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize output shape =  (320, 14)\n",
    "# r3 1 : encodings are ready\n",
    "# encodings.shape (320, 14)\n",
    "# CFO_off = 0.010980410815179016\n",
    "# r3 2 : channel effect added\n",
    "# all_y.shape (17980,)\n",
    "\n",
    "\n",
    "def r3ki3g_imitate_stch(encodings):\n",
    "    encRsh = tf.reshape(encodings,[-1])\n",
    "    ones = tf.ones( ( 17980-len(encRsh) ,) )\n",
    "    return tf.concat((encRsh,ones),axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig process - step by step\n",
    "\n",
    "1. Train an encoder somewhere else and import it to the Sequence decoder\n",
    "2. Train the Sequence decoder (smarter version of usual RX decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained modelmaker:\n",
    "\n",
    "http://localhost:8888/notebooks/PROJECTS/Deep-Learning-for-End-to-End-Over-the-Air-Communications/Fading%20-%20Nakagami%20Sumulation%20-%20Tensorflow/Making-pretrained-encoders-for-sequence-decoder-part.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.165989Z",
     "start_time": "2024-04-18T05:17:15.114025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load pre-tarined encoder\n",
    "\n",
    "\n",
    "savedir = r\"D:\\ENTC\\PROJECTS\\Deep-Learning-for-End-to-End-Over-the-Air-Communications\\Fading - Nakagami Sumulation - Tensorflow\\saved_encoders_for_Sequence_decoder\"\n",
    "\n",
    "\n",
    "absolute_path_for_encoder_model =  f'{savedir}\\\\r3ki3gEnc.h5'\n",
    "required_encoder_wo_l2norm = tf.keras.models.load_model(absolute_path_for_encoder_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.225723Z",
     "start_time": "2024-04-18T05:17:15.165989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3 01 statewidth init: 8\n"
     ]
    }
   ],
   "source": [
    "encoder = Sequential([\n",
    "\n",
    "           required_encoder_wo_l2norm,\n",
    "            L2Normalization(name=\"normalization_layer\")\n",
    "\n",
    "            ])\n",
    "\n",
    "\n",
    "# compiling is useless when we transfer teh flow to next model\n",
    "# encoder.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "#                     loss=\"mse\")\n",
    "\n",
    "\n",
    "\n",
    "stochastic_channel = StochasticChannelModel(NUM_CHANNEL_USES,\n",
    "                                                 block_size,\n",
    "                                                 channel_parameters)\n",
    "\n",
    "# Sequence decoder\n",
    "seq_decoder = SequenceDecoder(take_prev_phase_state=True)\n",
    "\n",
    "criterion = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer =  Adam(learning_rate=1e-2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.241110Z",
     "start_time": "2024-04-18T05:17:15.225723Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT \n",
    "# train step is called at end\n",
    "\n",
    "\n",
    "def train_step(x_train,y_train,):\n",
    "\n",
    "        x, y =  tf.cast(x_train,tf.float32) ,   tf.cast(y_train,tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        temp_prev_state_PE = tf.cast(tf.constant(np.zeros((1,8))),tf.float32)\n",
    "        temp_prev_state_FE = tf.cast(tf.constant(np.zeros((1,8))),tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.GradientTape(persistent=False) as tape:\n",
    "\n",
    "            encodings = encoder(x,\n",
    "                                     training=False)\n",
    "            \n",
    "            #print(\"r3 1 : encodings are ready\")\n",
    "            #print(\"encodings.shape\",encodings.shape)\n",
    "          \n",
    "\n",
    "#             all_y = r3ki3g_imitate_stch(encodings)\n",
    "#             print(\" by pass worked\")\n",
    "            all_y = stochastic_channel(encodings)\n",
    "            #print(\"r3 2 : channel effect added\")\n",
    "            #print(\"all_y.shape\", all_y.shape)\n",
    "\n",
    "\n",
    "            slices = ExternalSlicer(all_y,)\n",
    "            #print(\"r3 3 : slices are ready for the RX decoder\")\n",
    "\n",
    "\n",
    "            Jl = []\n",
    "            for i in range(slices.shape[0]):\n",
    "                #print(\"iteration\" , i)\n",
    "                with tape.stop_recording():\n",
    "                    y_slice = tf.expand_dims(slices[i,:],axis=0) \n",
    "                    label = tf.expand_dims(y[i,:],axis=0)\n",
    "#                     print(\"label d type\", label.dtype)\n",
    "#                     print(\"label\", label)\n",
    "                    \n",
    "              \n",
    "            \n",
    "\n",
    "\n",
    "                output = seq_decoder(y_slice,\n",
    "                                          temp_prev_state_FE,\n",
    "                                          temp_prev_state_PE)\n",
    "\n",
    "                # update states as well\n",
    "                st_hat,state_FE,state_PE = output[0], output[1], output[2]\n",
    "                \n",
    "#                 print(\"s hat dtype\", st_hat.dtype)\n",
    "#                 print(\"s hat shape\", st_hat.shape)\n",
    "#                 print(\"s hat\", st_hat)\n",
    "                \n",
    "                loss = criterion(label,st_hat)\n",
    "                \n",
    "                Jl.append(loss)\n",
    "                #print(f'-- current -- loss : {loss} , J : {J}')\n",
    "\n",
    "\n",
    "                # pass teh state to next iterration\n",
    "                temp_prev_state_FE = state_FE\n",
    "                temp_prev_state_PE = state_PE\n",
    "                \n",
    "\n",
    "            J = tf.reduce_mean(Jl)\n",
    "            # backpropagate\n",
    "            \n",
    "            trainable_variables =  seq_decoder.trainable_variables # +  encoder.trainable_variables \n",
    "            #print(\"encoder.trainable_variables\",encoder.trainable_variables)\n",
    "            grads = tape.gradient(J,trainable_variables)\n",
    "            #print(\"grads are ready\")\n",
    "            optimizer.apply_gradients(zip(grads,trainable_variables))\n",
    "\n",
    "        #print(f'Epoch  : {_}/{epochs} --> Loss = {loss_acc}')\n",
    "\n",
    "\n",
    "\n",
    "        return {\"J\":J, \"status\": \"training epoch ended\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# jhist = []   \n",
    "# for epoch_i in range(100):\n",
    "#     hist  = train_step(x_train,y_train  )\n",
    "#     print(hist)\n",
    "#     jhist.append(hist[\"J\"])\n",
    "    \n",
    "#     if epoch_i%10==0:\n",
    "#         plt.plot(jhist)\n",
    "#         plt.show()\n",
    "    \n",
    "\n",
    "# plt.plot(jhist)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.256657Z",
     "start_time": "2024-04-18T05:17:15.246746Z"
    }
   },
   "outputs": [],
   "source": [
    "# check accuracy\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(x_train,y_train,):\n",
    "\n",
    "        x, y =  tf.cast(x_train,tf.float32) ,   tf.cast(y_train,tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        temp_prev_state_PE = tf.cast(tf.constant(np.zeros((1,8))),tf.float32)\n",
    "        temp_prev_state_FE = tf.cast(tf.constant(np.zeros((1,8))),tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        encodings = encoder(x,\n",
    "                                 training=True)\n",
    "\n",
    "        #print(\"r3 1 : encodings are ready\")\n",
    "        #print(\"encodings.shape\",encodings.shape)\n",
    "\n",
    "\n",
    "#             all_y = r3ki3g_imitate_stch(encodings)\n",
    "#             print(\" by pass worked\")\n",
    "        all_y = stochastic_channel(encodings)\n",
    "        #print(\"r3 2 : channel effect added\")\n",
    "        #print(\"all_y.shape\", all_y.shape)\n",
    "\n",
    "\n",
    "        slices = ExternalSlicer(all_y,)\n",
    "        #print(\"r3 3 : slices are ready for the RX decoder\")\n",
    "\n",
    "\n",
    "        preds = []\n",
    "        for i in range(slices.shape[0]):\n",
    "            #print(\"iteration\" , i)\n",
    "          \n",
    "            y_slice = tf.expand_dims(slices[i,:],axis=0) \n",
    "            label = tf.expand_dims(y[i,:],axis=0)\n",
    "#                     print(\"label d type\", label.dtype)\n",
    "#                     print(\"label\", label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            output = seq_decoder(y_slice,\n",
    "                                      temp_prev_state_FE,\n",
    "                                      temp_prev_state_PE)\n",
    "\n",
    "            # update states as well\n",
    "            st_hat,state_FE,state_PE = output[0], output[1], output[2]\n",
    "\n",
    "#                 print(\"s hat dtype\", st_hat.dtype)\n",
    "#                 print(\"s hat shape\", st_hat.shape)\n",
    "#                 print(\"s hat\", st_hat)\n",
    "\n",
    "            preds.append(np.argmax(st_hat[0]))\n",
    "            # pass teh state to next iterration\n",
    "            temp_prev_state_FE = state_FE\n",
    "            temp_prev_state_PE = state_PE\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "        bit_accuracy = np.mean(tf.constant(preds)==y_train[:,0])\n",
    "        return {\"bit accuracy\": bit_accuracy }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.272697Z",
     "start_time": "2024-04-18T05:17:15.258652Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate(x_train,y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.288606Z",
     "start_time": "2024-04-18T05:17:15.274194Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMXg7FqQsYoU",
    "outputId": "2de71e71-5be4-45dc-cb21-5f6044d91932"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.1875, 60)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17980/320,17980%320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATE to try different archi  >> save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need a function to train on differnt sequences (of length : 320 = block size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.304569Z",
     "start_time": "2024-04-18T05:17:15.289604Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(encoder,\n",
    "                    stochastic_channel,\n",
    "                    seq_decoder,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    state_width):\n",
    "    '''\n",
    "    1. generate a block of 320 random messages\n",
    "    2. fit that to the model (RNN) once\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    x_train =  tf.cast(tf.random.uniform((block_size,),minval=0,maxval=2**k),\n",
    "                  (tf.int32))\n",
    "    y_train =  tf.expand_dims(x_train,axis=1)\n",
    "    x_train =  tf.one_hot(x_train,depth=2**k)\n",
    "    \n",
    "    \n",
    "    x, y =  tf.cast(x_train,tf.float32) ,   tf.cast(y_train,tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp_prev_state_PE = tf.cast(tf.constant(np.zeros((1,state_width))),tf.float32)\n",
    "    temp_prev_state_FE = tf.cast(tf.constant(np.zeros((1,state_width))),tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "\n",
    "        encodings = encoder(x,\n",
    "                                 training=False)\n",
    "\n",
    "        #print(\"r3 1 : encodings are ready\")\n",
    "        #print(\"encodings.shape\",encodings.shape)\n",
    "\n",
    "\n",
    "#             all_y = r3ki3g_imitate_stch(encodings)\n",
    "#             print(\" by pass worked\")\n",
    "        all_y = stochastic_channel(encodings)\n",
    "        #print(\"r3 2 : channel effect added\")\n",
    "        #print(\"all_y.shape\", all_y.shape)\n",
    "\n",
    "\n",
    "        slices = ExternalSlicer(all_y,) # not learning\n",
    "        #print(\"r3 3 : slices are ready for the RX decoder\")\n",
    "\n",
    "\n",
    "        Jl = []\n",
    "        for i in range(slices.shape[0]):\n",
    "            #print(\"iteration\" , i)\n",
    "            with tape.stop_recording():\n",
    "                y_slice = tf.expand_dims(slices[i,:],axis=0) \n",
    "                label = tf.expand_dims(y[i,:],axis=0)\n",
    "#                     print(\"label d type\", label.dtype)\n",
    "#                     print(\"label\", label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            output = seq_decoder(y_slice,\n",
    "                                      temp_prev_state_FE,\n",
    "                                      temp_prev_state_PE)\n",
    "\n",
    "            # update states as well\n",
    "            st_hat,state_FE,state_PE = output[0], output[1], output[2]\n",
    "\n",
    "#                 print(\"s hat dtype\", st_hat.dtype)\n",
    "#                 print(\"s hat shape\", st_hat.shape)\n",
    "#                 print(\"s hat\", st_hat)\n",
    "\n",
    "            loss = criterion(label,st_hat)\n",
    "\n",
    "            Jl.append(loss)\n",
    "            #print(f'-- current -- loss : {loss} , J : {J}')\n",
    "\n",
    "\n",
    "            # pass teh state to next iterration\n",
    "            temp_prev_state_FE = state_FE\n",
    "            temp_prev_state_PE = state_PE\n",
    "\n",
    "\n",
    "        J = tf.reduce_mean(Jl)\n",
    "        # backpropagate\n",
    "\n",
    "        trainable_variables =  seq_decoder.trainable_variables # +  encoder.trainable_variables \n",
    "        #print(\"encoder.trainable_variables\",encoder.trainable_variables)\n",
    "        grads = tape.gradient(J,trainable_variables)\n",
    "        #print(\"grads are ready\")\n",
    "        optimizer.apply_gradients(zip(grads,trainable_variables))\n",
    "\n",
    "    #print(f'Epoch  : {_}/{epochs} --> Loss = {loss_acc}')\n",
    "\n",
    "\n",
    "\n",
    "    return J.numpy().tolist()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.320225Z",
     "start_time": "2024-04-18T05:17:15.305559Z"
    }
   },
   "outputs": [],
   "source": [
    "def complete_experiment(n_front_dense_FE,\n",
    "                        n_front_dense_PE,\n",
    "                        n_front_dense_RxDec,\n",
    "                        state_width,n_epochs):\n",
    "    \n",
    "\n",
    "\n",
    "    encoder = Sequential([\n",
    "\n",
    "               required_encoder_wo_l2norm,\n",
    "                L2Normalization(name=\"normalization_layer\")\n",
    "\n",
    "                ])\n",
    "\n",
    "\n",
    "    # compiling is useless when we transfer teh flow to next model\n",
    "    # encoder.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "    #                     loss=\"mse\")\n",
    "\n",
    "\n",
    "\n",
    "    stochastic_channel = StochasticChannelModel(NUM_CHANNEL_USES,\n",
    "                                                     block_size,\n",
    "                                                     channel_parameters)\n",
    "\n",
    "    # Sequence decoder\n",
    "    seq_decoder = SequenceDecoder(take_prev_phase_state=True,\n",
    "                                  n_front_dense_FE=n_front_dense_FE,\n",
    "                                  n_front_dense_PE=n_front_dense_PE,\n",
    "                                  n_front_dense_RxDec=n_front_dense_RxDec,\n",
    "                                  state_width=state_width)\n",
    "\n",
    "    criterion = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer =  Adam(learning_rate=1e-2)\n",
    "    \n",
    "    # now loop for given amout of epochs\n",
    "    J_list = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'current epoch = {epoch+1} / {n_epochs}',end='\\r')\n",
    "        J_epoch =  train_one_epoch(encoder,\n",
    "                        stochastic_channel,\n",
    "                        seq_decoder,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        state_width)\n",
    "            \n",
    "        J_list.append(J_epoch)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "            \n",
    "    return {\n",
    "        \n",
    "        \"n_front_dense_FE\":n_front_dense_FE,\n",
    "                        \"n_front_dense_PE\":n_front_dense_PE,\n",
    "                        \"n_front_dense_RxDec\":n_front_dense_RxDec,\n",
    "                        \"state_width\":state_width,\n",
    "                        \"n_epochs\":n_epochs,\n",
    "                        \"J_list\":J_list,\n",
    "                        \"time_taken\":time_taken\n",
    "        \n",
    "        \n",
    "    }\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.335604Z",
     "start_time": "2024-04-18T05:17:15.322298Z"
    }
   },
   "outputs": [],
   "source": [
    "archi_res_dir = r\"D:\\ENTC\\PROJECTS\\Deep-Learning-for-End-to-End-Over-the-Air-Communications\\Sequence Decoder\\archi_tests_performance_lr_1e-2_part_2\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:15.352334Z",
     "start_time": "2024-04-18T05:17:15.338604Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "sys.exit(0)\n",
    "N_EPOCHS_EXPERIMENT = 10\n",
    "for n_front_dense_FE in [2,4,8]:\n",
    "    for n_front_dense_PE in [2,8,16,32]:\n",
    "        for n_front_dense_RxDec in [4,8,16]:\n",
    "            for state_width in [4,8,16]:\n",
    "                \n",
    "                    try:\n",
    "                        summary = complete_experiment(n_front_dense_FE=n_front_dense_FE,\n",
    "                                            n_front_dense_PE=n_front_dense_PE,\n",
    "                                            n_front_dense_RxDec=n_front_dense_RxDec,\n",
    "                                            state_width=state_width,n_epochs=N_EPOCHS_EXPERIMENT)\n",
    "\n",
    "                        filename = str(time.time()) + '.json'\n",
    "                        absfilepath = os.path.join(archi_res_dir,filename)\n",
    "                        with open(absfilepath,\"w\") as file:\n",
    "                            json.dump(summary,file)\n",
    "                        \n",
    "                        print(f'time taken = {summary[\"time_taken\"]}')\n",
    "                    \n",
    "                    except:\n",
    "                        print(\"error occured during:\")\n",
    "                        print({\"n_front_dense_FE\":n_front_dense_FE,\n",
    "                                            \"n_front_dense_PE\":1,\n",
    "                                              \"n_front_dense_RxDec\":2,\n",
    "                                            \"state_width\":8,\n",
    "                               \"n_epochs\":N_EPOCHS_EXPERIMENT})\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:17:27.882401Z",
     "start_time": "2024-04-18T05:17:23.398800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3 01 statewidth init: 4\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n",
      "r3 02 prev_state_FE shape in call: (1, 4)\n",
      "r3 03 sliced_y shape in call: (1, 120)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_front_dense_FE': 1,\n",
       " 'n_front_dense_PE': 1,\n",
       " 'n_front_dense_RxDec': 2,\n",
       " 'state_width': 4,\n",
       " 'n_epochs': 1,\n",
       " 'J_list': [2.7732510566711426],\n",
       " 'time_taken': 4.415235996246338}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_experiment(n_front_dense_FE=1,\n",
    "                    n_front_dense_PE=1,\n",
    "                    n_front_dense_RxDec=2,\n",
    "                    state_width=4,\n",
    "                    n_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
